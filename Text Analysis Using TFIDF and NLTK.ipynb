{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0e189cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e59227db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Aditya\n",
      "[nltk_data]     Yadav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aditya\n",
      "[nltk_data]     Yadav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Aditya\n",
      "[nltk_data]     Yadav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aditya Yadav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c629e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"The new smartphone release has been a game-changer! The camera quality is stunning, and the battery life lasts all day. However, the price feels a bit too high for the features offered. Overall, it's a solid device with a sleek design and smooth performance. I would recommend it to anyone looking for a premium experience, but if you're on a budget, you might want to explore other options.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a51df8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The new smartphone release has been a game-changer!', 'The camera quality is stunning, and the battery life lasts all day.', 'However, the price feels a bit too high for the features offered.', \"Overall, it's a solid device with a sleek design and smooth performance.\", \"I would recommend it to anyone looking for a premium experience, but if you're on a budget, you might want to explore other options.\"]\n"
     ]
    }
   ],
   "source": [
    "#Sentence Tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_text= sent_tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8c68878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'new', 'smartphone', 'release', 'has', 'been', 'a', 'game-changer', '!', 'The', 'camera', 'quality', 'is', 'stunning', ',', 'and', 'the', 'battery', 'life', 'lasts', 'all', 'day', '.', 'However', ',', 'the', 'price', 'feels', 'a', 'bit', 'too', 'high', 'for', 'the', 'features', 'offered', '.', 'Overall', ',', 'it', \"'s\", 'a', 'solid', 'device', 'with', 'a', 'sleek', 'design', 'and', 'smooth', 'performance', '.', 'I', 'would', 'recommend', 'it', 'to', 'anyone', 'looking', 'for', 'a', 'premium', 'experience', ',', 'but', 'if', 'you', \"'re\", 'on', 'a', 'budget', ',', 'you', 'might', 'want', 'to', 'explore', 'other', 'options', '.']\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word=word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22c4d357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'him', 'doing', 'why', 'nor', 's', 'in', 'there', 'isn', 'theirs', 'their', 'these', 'll', 'won', 'd', \"shouldn't\", \"we'll\", 'before', 'if', \"aren't\", 'my', 've', 'what', \"they're\", 'own', \"it's\", 'wouldn', \"we're\", 'very', 'for', \"he's\", 'having', \"isn't\", 'out', 'here', \"she's\", 'so', 'has', \"mustn't\", 'against', 'been', 'was', 'over', 'do', \"weren't\", 'up', \"you're\", 'hasn', 'were', 'being', 'them', 'shan', 'off', 'himself', 'how', \"i've\", 'ourselves', \"didn't\", 'to', 'further', 'all', 'm', \"wouldn't\", \"you'd\", \"i'll\", 'until', 'haven', 'just', \"we'd\", \"won't\", \"they'll\", 'itself', 'that', 'couldn', 'once', 'after', 'doesn', 'this', 'it', \"it'd\", 'are', 'who', 'hadn', 't', 'hers', \"you'll\", 'where', 'will', 'same', \"wasn't\", 'yourself', 'yours', 'they', 'should', \"it'll\", 'shouldn', 'any', \"don't\", 'me', \"she'll\", 'wasn', 'ma', 'no', 'with', 'his', 'am', 'don', 'from', 'by', 'of', 'other', 'few', \"he'll\", 'our', \"hadn't\", 'each', \"you've\", 'you', 'between', \"mightn't\", 'a', \"needn't\", 'she', 'above', 'does', \"we've\", 'mustn', \"hasn't\", 'did', \"that'll\", \"doesn't\", 'i', 'myself', \"shan't\", \"i'm\", 'then', 'o', 'weren', 'mightn', 'now', 'had', 'only', 'is', 'we', 'at', 'during', 'those', 'an', \"they'd\", 'be', \"haven't\", 're', 'herself', 'both', 'as', 'too', 'under', \"i'd\", 'y', 'her', \"she'd\", 'but', 'about', \"couldn't\", 'because', 'needn', 'which', 'whom', 'through', 'aren', 'can', 'he', 'on', 'the', 'your', \"he'd\", 'again', 'ours', 'have', 'most', 'didn', 'its', 'not', \"should've\", 'ain', 'while', 'or', 'yourselves', 'and', 'below', 'such', 'more', 'down', \"they've\", 'themselves', 'some', 'than', 'into', 'when'}\n"
     ]
    }
   ],
   "source": [
    "# print stop words of English\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e773b1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['the', 'new', 'smartphone', 'release', 'has', 'been', 'a', 'game', 'changer', 'the', 'camera', 'quality', 'is', 'stunning', 'and', 'the', 'battery', 'life', 'lasts', 'all', 'day', 'however', 'the', 'price', 'feels', 'a', 'bit', 'too', 'high', 'for', 'the', 'features', 'offered', 'overall', 'it', 's', 'a', 'solid', 'device', 'with', 'a', 'sleek', 'design', 'and', 'smooth', 'performance', 'i', 'would', 'recommend', 'it', 'to', 'anyone', 'looking', 'for', 'a', 'premium', 'experience', 'but', 'if', 'you', 're', 'on', 'a', 'budget', 'you', 'might', 'want', 'to', 'explore', 'other', 'options']\n",
      "\n",
      " Filterd Sentence: ['new', 'smartphone', 'release', 'game', 'changer', 'camera', 'quality', 'stunning', 'battery', 'life', 'lasts', 'day', 'however', 'price', 'feels', 'bit', 'high', 'features', 'offered', 'overall', 'solid', 'device', 'sleek', 'design', 'smooth', 'performance', 'would', 'recommend', 'anyone', 'looking', 'premium', 'experience', 'budget', 'might', 'want', 'explore', 'options']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text= re.sub('[^a-zA-Z]', ' ',text)\n",
    "tokens = word_tokenize(text.lower())\n",
    "filtered_text=[]\n",
    "for w in tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_text.append(w)\n",
    "print(\"Tokenized Sentence:\",tokens)\n",
    "print(\"\\n Filterd Sentence:\",filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c092e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new', 'smartphon', 'releas', 'game', 'changer', 'camera', 'qualiti', 'stun', 'batteri', 'life', 'last', 'day', 'howev', 'price', 'feel', 'bit', 'high', 'featur', 'offer', 'overal', 'solid', 'devic', 'sleek', 'design', 'smooth', 'perform', 'would', 'recommend', 'anyon', 'look', 'premium', 'experi', 'budget', 'might', 'want', 'explor', 'option']\n"
     ]
    }
   ],
   "source": [
    "#perform stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "ps =PorterStemmer()\n",
    "rootWord=[]\n",
    "for w in filtered_text:\n",
    "    rootWord.append(ps.stem(w))\n",
    "print(rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1b74890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for The is The\n",
      "Lemma for new is new\n",
      "Lemma for smartphone is smartphone\n",
      "Lemma for release is release\n",
      "Lemma for has is ha\n",
      "Lemma for been is been\n",
      "Lemma for a is a\n",
      "Lemma for game is game\n",
      "Lemma for changer is changer\n",
      "Lemma for The is The\n",
      "Lemma for camera is camera\n",
      "Lemma for quality is quality\n",
      "Lemma for is is is\n",
      "Lemma for stunning is stunning\n",
      "Lemma for and is and\n",
      "Lemma for the is the\n",
      "Lemma for battery is battery\n",
      "Lemma for life is life\n",
      "Lemma for lasts is last\n",
      "Lemma for all is all\n",
      "Lemma for day is day\n",
      "Lemma for However is However\n",
      "Lemma for the is the\n",
      "Lemma for price is price\n",
      "Lemma for feels is feel\n",
      "Lemma for a is a\n",
      "Lemma for bit is bit\n",
      "Lemma for too is too\n",
      "Lemma for high is high\n",
      "Lemma for for is for\n",
      "Lemma for the is the\n",
      "Lemma for features is feature\n",
      "Lemma for offered is offered\n",
      "Lemma for Overall is Overall\n",
      "Lemma for it is it\n",
      "Lemma for s is s\n",
      "Lemma for a is a\n",
      "Lemma for solid is solid\n",
      "Lemma for device is device\n",
      "Lemma for with is with\n",
      "Lemma for a is a\n",
      "Lemma for sleek is sleek\n",
      "Lemma for design is design\n",
      "Lemma for and is and\n",
      "Lemma for smooth is smooth\n",
      "Lemma for performance is performance\n",
      "Lemma for I is I\n",
      "Lemma for would is would\n",
      "Lemma for recommend is recommend\n",
      "Lemma for it is it\n",
      "Lemma for to is to\n",
      "Lemma for anyone is anyone\n",
      "Lemma for looking is looking\n",
      "Lemma for for is for\n",
      "Lemma for a is a\n",
      "Lemma for premium is premium\n",
      "Lemma for experience is experience\n",
      "Lemma for but is but\n",
      "Lemma for if is if\n",
      "Lemma for you is you\n",
      "Lemma for re is re\n",
      "Lemma for on is on\n",
      "Lemma for a is a\n",
      "Lemma for budget is budget\n",
      "Lemma for you is you\n",
      "Lemma for might is might\n",
      "Lemma for want is want\n",
      "Lemma for to is to\n",
      "Lemma for explore is explore\n",
      "Lemma for other is other\n",
      "Lemma for options is option\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer =WordNetLemmatizer() \n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w,wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f139432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT')]\n",
      "[('new', 'JJ')]\n",
      "[('smartphone', 'NN')]\n",
      "[('release', 'NN')]\n",
      "[('has', 'VBZ')]\n",
      "[('been', 'VBN')]\n",
      "[('a', 'DT')]\n",
      "[('game', 'NN')]\n",
      "[('changer', 'NN')]\n",
      "[('The', 'DT')]\n",
      "[('camera', 'NN')]\n",
      "[('quality', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('stunning', 'VBG')]\n",
      "[('and', 'CC')]\n",
      "[('the', 'DT')]\n",
      "[('battery', 'NN')]\n",
      "[('life', 'NN')]\n",
      "[('lasts', 'NNS')]\n",
      "[('all', 'DT')]\n",
      "[('day', 'NN')]\n",
      "[('However', 'RB')]\n",
      "[('the', 'DT')]\n",
      "[('price', 'NN')]\n",
      "[('feels', 'NNS')]\n",
      "[('a', 'DT')]\n",
      "[('bit', 'NN')]\n",
      "[('too', 'RB')]\n",
      "[('high', 'JJ')]\n",
      "[('for', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('features', 'NNS')]\n",
      "[('offered', 'VBN')]\n",
      "[('Overall', 'JJ')]\n",
      "[('it', 'PRP')]\n",
      "[('s', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('solid', 'JJ')]\n",
      "[('device', 'NN')]\n",
      "[('with', 'IN')]\n",
      "[('a', 'DT')]\n",
      "[('sleek', 'NN')]\n",
      "[('design', 'NN')]\n",
      "[('and', 'CC')]\n",
      "[('smooth', 'NN')]\n",
      "[('performance', 'NN')]\n",
      "[('I', 'PRP')]\n",
      "[('would', 'MD')]\n",
      "[('recommend', 'NN')]\n",
      "[('it', 'PRP')]\n",
      "[('to', 'TO')]\n",
      "[('anyone', 'NN')]\n",
      "[('looking', 'VBG')]\n",
      "[('for', 'IN')]\n",
      "[('a', 'DT')]\n",
      "[('premium', 'NN')]\n",
      "[('experience', 'NN')]\n",
      "[('but', 'CC')]\n",
      "[('if', 'IN')]\n",
      "[('you', 'PRP')]\n",
      "[('re', 'NN')]\n",
      "[('on', 'IN')]\n",
      "[('a', 'DT')]\n",
      "[('budget', 'NN')]\n",
      "[('you', 'PRP')]\n",
      "[('might', 'MD')]\n",
      "[('want', 'NN')]\n",
      "[('to', 'TO')]\n",
      "[('explore', 'NN')]\n",
      "[('other', 'JJ')]\n",
      "[('options', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "words=word_tokenize(text)\n",
    "for word in words:\n",
    "    print(nltk.pos_tag([word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d07a3c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create representation of document by calculating TFIDF \n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b950d058",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentA = 'Jupiter is the largest Planet'\n",
    "documentB = 'Mars is the fourth planet from the Sun'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39afd4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of words\n",
    "bagOfWordsA = documentA.split(' ')\n",
    "bagOfWordsB = documentB.split(' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58b29116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Jupiter',\n",
       " 'Mars',\n",
       " 'Planet',\n",
       " 'Sun',\n",
       " 'fourth',\n",
       " 'from',\n",
       " 'is',\n",
       " 'largest',\n",
       " 'planet',\n",
       " 'the'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueWords =set(bagOfWordsA).union(set(bagOfWordsB)) \n",
    "uniqueWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "538c3263",
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsA:\n",
    "    numOfWordsA[word] += 1\n",
    "numOfWordsB = dict.fromkeys(uniqueWords,0)\n",
    "for word in bagOfWordsB:\n",
    "    numOfWordsB[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c118acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount =len(bagOfWords) \n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count /float(bagOfWordsCount)\n",
    "    return tfDict\n",
    "tfA = computeTF(numOfWordsA,bagOfWordsA)\n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe0ee162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 0.2,\n",
       " 'Mars': 0.0,\n",
       " 'fourth': 0.0,\n",
       " 'largest': 0.2,\n",
       " 'from': 0.0,\n",
       " 'Jupiter': 0.2,\n",
       " 'the': 0.2,\n",
       " 'Planet': 0.2,\n",
       " 'planet': 0.0,\n",
       " 'Sun': 0.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76ebf632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 0.125,\n",
       " 'Mars': 0.125,\n",
       " 'fourth': 0.125,\n",
       " 'largest': 0.0,\n",
       " 'from': 0.125,\n",
       " 'Jupiter': 0.0,\n",
       " 'the': 0.25,\n",
       " 'Planet': 0.0,\n",
       " 'planet': 0.125,\n",
       " 'Sun': 0.125}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a69fc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 0.0,\n",
       " 'Mars': 0.6931471805599453,\n",
       " 'fourth': 0.6931471805599453,\n",
       " 'largest': 0.6931471805599453,\n",
       " 'from': 0.6931471805599453,\n",
       " 'Jupiter': 0.6931471805599453,\n",
       " 'the': 0.0,\n",
       " 'Planet': 0.6931471805599453,\n",
       " 'planet': 0.6931471805599453,\n",
       " 'Sun': 0.6931471805599453}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    idfDict = dict.fromkeys(documents[0].keys(),0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N /float(val))\n",
    "    return idfDict\n",
    "idfs = computeIDF([numOfWordsA,numOfWordsB])\n",
    "idfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dee3def1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is</th>\n",
       "      <th>Mars</th>\n",
       "      <th>fourth</th>\n",
       "      <th>largest</th>\n",
       "      <th>from</th>\n",
       "      <th>Jupiter</th>\n",
       "      <th>the</th>\n",
       "      <th>Planet</th>\n",
       "      <th>planet</th>\n",
       "      <th>Sun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.138629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.086643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    is      Mars    fourth   largest      from   Jupiter  the    Planet  \\\n",
       "0  0.0  0.000000  0.000000  0.138629  0.000000  0.138629  0.0  0.138629   \n",
       "1  0.0  0.086643  0.086643  0.000000  0.086643  0.000000  0.0  0.000000   \n",
       "\n",
       "     planet       Sun  \n",
       "0  0.000000  0.000000  \n",
       "1  0.086643  0.086643  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute the term TF/IDF for all words.\n",
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf\n",
    "tfidfA = computeTFIDF(tfA,idfs)\n",
    "tfidfB = computeTFIDF(tfB,idfs)\n",
    "df = pd.DataFrame([tfidfA,tfidfB])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cf63a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
